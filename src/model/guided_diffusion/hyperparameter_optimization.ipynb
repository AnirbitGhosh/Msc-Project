{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import optuna\n",
    "\n",
    "# VOL_PATH = \"/vol/bitbucket/ag323\"\n",
    "VOL_PATH = \"C:/Users/Anirbit/Desktop/MSc/Ind Project/Msc-Project\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Free Guidance - Diffusion Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=1000):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "    def forward(self, x, t_emb, c_emb, mask):\n",
    "        for layer in self:\n",
    "            if(isinstance(layer, TimestepBlock)):\n",
    "                x = layer(x, t_emb, c_emb, mask)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "\n",
    "def norm_layer(channels):\n",
    "    return nn.GroupNorm(32, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(TimestepBlock):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, cond_channels, dropout, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            norm_layer(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        )\n",
    "        \n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.cond_conv = nn.Sequential(\n",
    "            nn.Conv2d(cond_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            norm_layer(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        )\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t, cond_img, mask):\n",
    "        h = self.conv1(x)\n",
    "        emb_t = self.time_emb(t)\n",
    "        emb_cond = self.cond_conv(cond_img) * mask[:, None, None, None]\n",
    "        h += (emb_t[:, :, None, None] + emb_cond)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        return h + self.shortcut(x)\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert channels % num_heads == 0\n",
    "        \n",
    "        self.norm = norm_layer(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q, k, v = qkv.reshape(B *  self.num_heads, -1, H * W).chunk(3, dim=1)\n",
    "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
    "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
    "        h = h.reshape(B, -1, H, W)\n",
    "        h = self.proj(h)\n",
    "        return h + x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, padding=1, stride=2)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=2,\n",
    "                 cond_channels=1,\n",
    "                 model_channels=128,\n",
    "                 out_channels=2,\n",
    "                 num_res_blocks=2, \n",
    "                 attention_resolutions=(8, 16),\n",
    "                 dropout=0,\n",
    "                 channel_mult=(1, 2, 2, 2),\n",
    "                 conv_resample=True,\n",
    "                 num_heads=4,\n",
    "                 down_block_kernel=3,\n",
    "                 mid_block_kernel=3,\n",
    "                 up_block_kernel=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.cond_channels = cond_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions,\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult,\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        self.down_block_kernel = down_block_kernel\n",
    "        self.up_block_kernel = up_block_kernel\n",
    "        self.mid_block_kernel = mid_block_kernel\n",
    "        \n",
    "        # time embedding\n",
    "        time_emb_dim = model_channels * 4\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # down blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv2d(in_channels, model_channels, kernel_size=self.down_block_kernel, padding=1))\n",
    "        ])\n",
    "        down_block_channels = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [ResidualBlock(ch, model_channels * mult, time_emb_dim, cond_channels, dropout, kernel_size=self.down_block_kernel)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                down_block_channels.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                self.down_blocks.append(TimestepEmbedSequential(DownSample(ch, conv_resample)))\n",
    "                down_block_channels.append(ch)\n",
    "                ds *= 2\n",
    "        \n",
    "        # middle blocks\n",
    "        self.middle_blocks = TimestepEmbedSequential(\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cond_channels, dropout, kernel_size=self.mid_block_kernel),\n",
    "            AttentionBlock(ch, num_heads),\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cond_channels, dropout, kernel_size=self.mid_block_kernel)\n",
    "        )\n",
    "        \n",
    "        # up blocks\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for level, mult in enumerate(channel_mult[::-1]):\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                layers = [\n",
    "                    ResidualBlock(ch + down_block_channels.pop(), model_channels * mult, time_emb_dim, cond_channels, dropout, kernel_size=self.up_block_kernel)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                if level != len(channel_mult) - 1 and i == num_res_blocks:\n",
    "                    layers.append(UpSample(ch, conv_resample))\n",
    "                    ds //= 2\n",
    "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                \n",
    "        self.out = nn.Sequential(\n",
    "            norm_layer(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, timesteps, cond_img, mask):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x H x W] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param cond_img: a [N x cond_C x H x W] Tensor of conditional images.\n",
    "        :param mask: a 1-D batch of conditioned/unconditioned.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        # time step embedding\n",
    "        t_emb = self.time_emb(timestep_embedding(timesteps, dim=self.model_channels))\n",
    "        \n",
    "        # down step\n",
    "        h = x\n",
    "        for module in self.down_blocks:\n",
    "            if cond_img.shape[2:] != h.shape[2:]:\n",
    "                cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "            h = module(h, t_emb, cond_img, mask)\n",
    "            hs.append(h)\n",
    "        # mid stage\n",
    "        if cond_img.shape[2:] != h.shape[2:]:\n",
    "            cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "        h = self.middle_blocks(h, t_emb, cond_img, mask)\n",
    "        \n",
    "        # up stage\n",
    "        for module in self.up_blocks:\n",
    "            h_skip = hs.pop()\n",
    "            \n",
    "            if h.shape[2:] != h_skip.shape[2:]:\n",
    "                h = F.interpolate(h, size=h_skip.shape[2:], mode='nearest')\n",
    "\n",
    "            if cond_img.shape[2:] != h.shape[2:]:\n",
    "                cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "\n",
    "            cat_in = torch.cat([h, h_skip], dim=1)\n",
    "            h = module(cat_in, t_emb, cond_img, mask)\n",
    "        \n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta schedule\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    betas = torch.sigmoid(betas) / (betas.max() - betas.min()) * (0.02 - betas.min()) / 10\n",
    "    return betas\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "class GaussianDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000,\n",
    "        beta_schedule='linear',\n",
    "    ):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            betas = sigmoid_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule {beta_schedule}')\n",
    "        \n",
    "        self.betas = betas\n",
    "        \n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "        \n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "        \n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]])\n",
    "        )\n",
    "        \n",
    "        self.posterior_mean_coef1 = (\n",
    "            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    # get the param of given timestep t\n",
    "    def _extract(self, a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.to(t.device).gather(0, t).float()\n",
    "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "        return out\n",
    "    \n",
    "    # forward diffusion : q(x_t | x_0)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    # mean and variance of q(x_t | x_0)\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "    \n",
    "    # mean and variance of diffusion posterior: q(x_{t-1} | x_t, x_0)\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "    \n",
    "    # compute x_0 from x_t and pred noise: reverse of q_sample\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    # compute predicted mean and variance of p(x_{t-1} | x_t) \n",
    "    def p_mean_variance(self, model, x_t, t, cond_img, w, clip_denoised=True):\n",
    "        device = next(model.parameters()).device\n",
    "        batch_size = x_t.shape[0]\n",
    "        \n",
    "        # noise prediction from model\n",
    "        pred_noise_cond = model(x_t, t, cond_img, torch.ones(batch_size).int().to(device))\n",
    "        pred_noise_uncond = model(x_t, t, cond_img, torch.zeros(batch_size).int().to(device))\n",
    "        pred_noise = (1 + w) * pred_noise_cond - w * pred_noise_uncond\n",
    "        \n",
    "        # get predicted x_0\n",
    "        x_recon = self.predict_start_from_noise(x_t, t, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x_recon = torch.clamp(x_recon, min=-1., max=1.)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior_mean_variance(x_recon, x_t, t)\n",
    "        \n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "    \n",
    "    # denoise step: sample x_{t-1} from x_t and pred noise\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t, cond_img, w, clip_denoised=True):\n",
    "        # pred mean and variance\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t, cond_img, w, clip_denoised=clip_denoised)\n",
    "        \n",
    "        noise = torch.randn_like(x_t)\n",
    "        # no noise when t = 0 \n",
    "        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n",
    "        # compute x_{t-1}\n",
    "        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img\n",
    "    \n",
    "    # denoise : reverse diffusion\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape, cond_img, w=2, clip_denoised=True):\n",
    "        batch_size = shape[0]\n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        # start from pure noise\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "        for i in tqdm(reversed(range(0, self.timesteps)), desc='sampling loop time step', total=self.timesteps):\n",
    "            img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long), cond_img, w, clip_denoised)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "    \n",
    "    # sample new images\n",
    "    @torch.no_grad\n",
    "    def sample(self, model, image_size, cond_img, batch_size=8, channels=3, w=2, clip_denoised=True):\n",
    "        return self.p_sample_loop(model, (batch_size, channels, image_size, image_size), cond_img, w, clip_denoised)\n",
    "    \n",
    "    # use ddim to sample\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        image_size,\n",
    "        cond_img,\n",
    "        batch_size=8,\n",
    "        channels=3,\n",
    "        ddim_timesteps=50,\n",
    "        w=2,\n",
    "        ddim_discr_method=\"uniform\",\n",
    "        ddim_eta=0.0,\n",
    "        clip_denoised=True):\n",
    "        \n",
    "        # make ddim timestep sequence\n",
    "        if ddim_discr_method == 'uniform':\n",
    "            c = self.timesteps // ddim_timesteps\n",
    "            ddim_timestep_seq = np.asarray(list(range(0, self.timesteps, c)))\n",
    "        elif ddim_discr_method == 'quad':\n",
    "            ddim_timestep_seq = (\n",
    "                (np.linspace(0, np.sqrt(self.timesteps * .8), ddim_timesteps)) ** 2\n",
    "            ).astype(int)\n",
    "        else:\n",
    "            raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
    "        # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "        ddim_timestep_seq = ddim_timestep_seq + 1\n",
    "        # previous sequence\n",
    "        ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        # start from pure noise (for each example in the batch)\n",
    "        sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)\n",
    "        seq_img = [sample_img.cpu().numpy()]   \n",
    "        \n",
    "        for i in tqdm(reversed(range(0, ddim_timesteps)), desc='sampling loop time step', total=ddim_timesteps):\n",
    "            t = torch.full((batch_size,), ddim_timestep_seq[i], device=device, dtype=torch.long)\n",
    "            prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=device, dtype=torch.long)\n",
    "            \n",
    "            # 1. get current and previous alpha_cumprod\n",
    "            alpha_cumprod_t = self._extract(self.alphas_cumprod, t, sample_img.shape)\n",
    "            alpha_cumprod_t_prev = self._extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
    "    \n",
    "            # 2. predict noise using model\n",
    "            pred_noise_cond = model(sample_img, t, cond_img, torch.ones(batch_size).int().cuda())\n",
    "            pred_noise_uncond = model(sample_img, t, cond_img, torch.zeros(batch_size).int().cuda())\n",
    "            pred_noise = (1+w)*pred_noise_cond - w*pred_noise_uncond\n",
    "            \n",
    "            # 3. get the predicted x_0\n",
    "            pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "            if clip_denoised:\n",
    "                pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
    "            \n",
    "            # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "            # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "            sigmas_t = ddim_eta * torch.sqrt(\n",
    "                (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
    "            \n",
    "            # 5. compute \"direction pointing to x_t\" of formula (12)\n",
    "            pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
    "            \n",
    "            # 6. compute x_{t-1} of formula (12)\n",
    "            x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
    "\n",
    "            sample_img = x_prev\n",
    "\n",
    "        return sample_img.cpu().numpy()\n",
    "    \n",
    "    # compute train losses\n",
    "    def train_losses(self, model, x_start, t, cond_img, mask_c):\n",
    "        # generate random noise\n",
    "        noise = torch.randn_like(x_start)\n",
    "        # get x_t\n",
    "        x_noisy = self.q_sample(x_start, t, noise=noise)\n",
    "        predicted_noise = model(x_noisy, t, cond_img, mask_c)\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, conditional_offset=5):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.conditional_offset = conditional_offset\n",
    "        self.cond_images = []\n",
    "        self.target_images = []\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        files = sorted([os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir) if f.endswith('.mpy')])\n",
    "        for file in files:\n",
    "            with open(file, 'rb') as f:\n",
    "                images = pickle.load(f)\n",
    "                if isinstance(images, list):\n",
    "                    images = np.array(images)\n",
    "                    \n",
    "                for img_idx in range(len(images) - self.conditional_offset):\n",
    "                    self.cond_images.append(images[img_idx])\n",
    "                    self.target_images.append(images[img_idx + self.conditional_offset])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cond_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond_image = self.cond_images[idx]\n",
    "        image = self.target_images[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            cond_image = self.transform(cond_image)\n",
    "\n",
    "        return image, cond_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "timesteps = 500\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomRotation(degrees=15, fill=0),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), fill=0),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "id_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(64)\n",
    "])\n",
    "\n",
    "DATA_DIR = os.path.join(VOL_PATH, \"diss_ag/data/simulated_bin_frames\")\n",
    "RESULT_DIR = os.path.join(VOL_PATH, \"diss_ag/results/\")\n",
    "CKPT_DIR = os.path.join(VOL_PATH, \"diss_ag/results/ckpts\") \n",
    "\n",
    "base_dataset = CondImageDataset(DATA_DIR, transform=id_transform)\n",
    "aug_dataset = CondImageDataset(DATA_DIR, transform=aug_transform)\n",
    "\n",
    "dataset = ConcatDataset([base_dataset, aug_dataset])\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Further split the train_dataset into training and validation sets\n",
    "val_size = int(0.1 * train_size)\n",
    "train_size = train_size - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "gaussian_diffusion = GaussianDiffusion(timesteps=500, beta_schedule='linear')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Unet(\n",
    "    in_channels=1,\n",
    "    cond_channels=1,\n",
    "    model_channels=96,\n",
    "    out_channels=1,\n",
    "    channel_mult=(1, 2, 2),\n",
    "    attention_resolutions=[],\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTUNA - hyperparameter optimization study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience, diffusion, p_uncound=0.2):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        time_start = time.time()\n",
    "        for step, (images, cond_images) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size = images.shape[0]\n",
    "            images = images.to(device).float()\n",
    "            cond_images = cond_images.to(device).float()\n",
    "\n",
    "            z_uncound = torch.rand(batch_size)\n",
    "            batch_mask = (z_uncound > p_uncound).int().to(device)\n",
    "\n",
    "            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()\n",
    "            loss = diffusion.train_losses(model, images, t, cond_images, batch_mask)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                time_end = time.time()\n",
    "                print(\"Epoch {}/{}\\t Step {}/{}\\t Loss {:.4f}\\t Time {:.2f}\".format(\n",
    "                    epoch + 1, epochs, step + 1, len(train_loader), loss.item(), time_end - time_start))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, diffusion)\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, loader, criterion, diffusion, p_uncound=0.2):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, cond_images = batch\n",
    "            batch_size = images.shape[0]\n",
    "            images = images.to(device).float()\n",
    "            cond_images = cond_images.to(device).float()\n",
    "\n",
    "            z_uncound = torch.rand(batch_size)\n",
    "            batch_mask = (z_uncound > p_uncound).int().to(device)\n",
    "\n",
    "            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()\n",
    "            noise = torch.randn_like(images)\n",
    "            x_noisy = diffusion.q_sample(images, t, noise=noise)\n",
    "            predicted_noise = model(x_noisy, t, cond_images, batch_mask)\n",
    "            loss = criterion(noise, predicted_noise)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model_channels = trial.suggest_int('model_channels', 64, 512, step=64)\n",
    "    num_res_blocks = trial.suggest_int('num_res_blocks', 1, 4)\n",
    "    attention_resolutions = trial.suggest_categorical('attention_resolutions', [(8,), (16,), (8, 16)])\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5, step=0.1)\n",
    "    channel_mult = trial.suggest_categorical('channel_mult', [(1, 2), (1, 2, 2), (1, 2, 4)])\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
    "    down_block_kernel_size = trial.suggest_int('down_block_kernel_size', 1, 5, step=2)\n",
    "    up_block_kernel_size = trial.suggest_int('up_block_kernel_size', 1, 5, step=2)\n",
    "    mid_block_kernel_size = trial.suggest_int('mid_block_kernel_size', 1, 5, step=2)\n",
    "    \n",
    "    model = Unet(\n",
    "        model_channels=model_channels,\n",
    "        num_res_blocks=num_res_blocks,\n",
    "        attention_resolutions=attention_resolutions,\n",
    "        dropout=dropout,\n",
    "        channel_mult=channel_mult,\n",
    "        num_heads=num_heads,\n",
    "        down_block_kernel_size=down_block_kernel_size,\n",
    "        up_block_kernel_size=up_block_kernel_size,\n",
    "        mid_block_kernel_size=mid_block_kernel_size\n",
    "    ).to(device)\n",
    "    \n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Assume train_loader and val_loader are defined globally or passed as arguments\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    diffusion = GaussianDiffusion(timesteps=1000)  # Replace 1000 with the appropriate number of timesteps\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience, diffusion)\n",
    "\n",
    "    test_loss = evaluate_model(model, test_loader, criterion, diffusion)\n",
    "    return test_loss\n",
    "\n",
    "# Create a study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
