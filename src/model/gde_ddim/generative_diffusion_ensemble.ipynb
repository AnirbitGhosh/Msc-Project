{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=1000):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "    def forward(self, x, t_emb, c_emb, mask):\n",
    "        for layer in self:\n",
    "            if(isinstance(layer, TimestepBlock)):\n",
    "                x = layer(x, t_emb, c_emb, mask)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "\n",
    "def norm_layer(channels):\n",
    "    return nn.GroupNorm(32, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(TimestepBlock):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, class_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            norm_layer(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.class_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(class_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            norm_layer(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t, c, mask):\n",
    "        \"\"\"\n",
    "        `x` has shape `[batch_size, in_dim, height, width]`\n",
    "        `t` has shape `[batch_size, time_dim]`\n",
    "        `c` has shape `[batch_size, class_dim]`\n",
    "        `mask` has shape `[batch_size, ]`\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        emb_t = self.time_emb(t)\n",
    "        emb_c = self.class_emb(c)*mask[:, None]\n",
    "        h += (emb_t[:, :, None, None] + emb_c[:, :, None, None])\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        return h + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert channels % num_heads == 0\n",
    "        \n",
    "        self.norm = norm_layer(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q, k, v = qkv.reshape(B *  self.num_heads, -1, H * W).chunk(3, dim=1)\n",
    "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
    "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
    "        h = h.reshape(B, -1, H, W)\n",
    "        h = self.proj(h)\n",
    "        return h + x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, padding=1, stride=2)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=2,\n",
    "                 model_channels=128,\n",
    "                 out_channels=2,\n",
    "                 num_res_blocks=2, \n",
    "                 attention_resolutions=(8, 16),\n",
    "                 dropout=0,\n",
    "                 channel_mult=(1, 2, 2, 2),\n",
    "                 conv_resample=True,\n",
    "                 num_heads=4, \n",
    "                 class_num=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions,\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult,\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        self.class_num = class_num\n",
    "        \n",
    "        # time embedding\n",
    "        time_emb_dim = model_channels*4\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # conditional (class) embedding\n",
    "        class_emb_dim = model_channels\n",
    "        self.class_emb = nn.Embedding(class_num, class_emb_dim)\n",
    "        \n",
    "        # down blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1))\n",
    "        ])\n",
    "        down_block_channels = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [ResidualBlock(ch, model_channels*mult, time_emb_dim, class_emb_dim, dropout)]\n",
    "                ch = model_channels*mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                down_block_channels.append(ch)\n",
    "            if level != len(channel_mult)-1:\n",
    "                self.down_blocks.append(TimestepEmbedSequential(DownSample(ch, conv_resample)))\n",
    "                down_block_channels.append(ch)\n",
    "                ds *= 2\n",
    "        \n",
    "        # middle blocks\n",
    "        self.middle_blocks = TimestepEmbedSequential(\n",
    "            ResidualBlock(ch, ch, time_emb_dim, class_emb_dim, dropout),\n",
    "            AttentionBlock(ch, num_heads),\n",
    "            ResidualBlock(ch, ch, time_emb_dim, class_emb_dim, dropout)\n",
    "        )\n",
    "        \n",
    "        # up blocks\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for level, mult in enumerate(channel_mult[::-1]):\n",
    "            for i in range(num_res_blocks+1):\n",
    "                layers = [\n",
    "                    ResidualBlock(ch+down_block_channels.pop(), model_channels*mult, time_emb_dim, class_emb_dim, dropout)\n",
    "                ]\n",
    "                ch = model_channels*mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                if level != len(channel_mult)-1 and i == num_res_blocks:\n",
    "                    layers.append(UpSample(ch, conv_resample))\n",
    "                    ds //=2\n",
    "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                \n",
    "        self.out = nn.Sequential(\n",
    "            norm_layer(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, timesteps, c, mask):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x H x W] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param c: a 1-D batch of classes.\n",
    "        :param mask: a 1-D batch of conditioned/unconditioned.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        # time step and class embedding\n",
    "        t_emb = self.time_emb(timestep_embedding(timesteps, dim=self.model_channels))\n",
    "        c_emb = self.class_emb(c)\n",
    "        \n",
    "        # down step\n",
    "        h = x\n",
    "        for module in self.down_blocks:\n",
    "            h = module(h, t_emb, c_emb, mask)\n",
    "            hs.append(h)\n",
    "        \n",
    "        # mid stage\n",
    "        h = self.middle_blocks(h, t_emb, c_emb, mask)\n",
    "        \n",
    "        # up stage\n",
    "        for module in self.up_blocks:\n",
    "            cat_in = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = module(cat_in, t_emb, c_emb, mask)\n",
    "            \n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta schedule\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    betas = torch.sigmoid(betas) / (betas.max() - betas.min()) * (0.02 - betas.min()) / 10\n",
    "    return betas\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
