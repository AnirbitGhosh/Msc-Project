{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet and Gaussian Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=1000):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "    def forward(self, x, t_emb, c_emb, mask):\n",
    "        for layer in self:\n",
    "            if(isinstance(layer, TimestepBlock)):\n",
    "                x = layer(x, t_emb, c_emb, mask)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "\n",
    "def norm_layer(channels):\n",
    "    return nn.GroupNorm(32, channels)\n",
    "\n",
    "class ResidualBlock(TimestepBlock):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, cond_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            norm_layer(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.cond_conv = nn.Sequential(\n",
    "            nn.Conv2d(cond_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            norm_layer(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t, cond_img, mask):\n",
    "        h = self.conv1(x)\n",
    "        emb_t = self.time_emb(t)\n",
    "        emb_cond = self.cond_conv(cond_img) * mask[:, None, None, None]\n",
    "        h += (emb_t[:, :, None, None] + emb_cond)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        return h + self.shortcut(x)\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert channels % num_heads == 0\n",
    "        \n",
    "        self.norm = norm_layer(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q, k, v = qkv.reshape(B *  self.num_heads, -1, H * W).chunk(3, dim=1)\n",
    "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
    "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
    "        h = h.reshape(B, -1, H, W)\n",
    "        h = self.proj(h)\n",
    "        return h + x\n",
    "    \n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, padding=1, stride=2)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=2,\n",
    "                 cond_channels=1,\n",
    "                 model_channels=128,\n",
    "                 out_channels=2,\n",
    "                 num_res_blocks=2, \n",
    "                 attention_resolutions=(8, 16),\n",
    "                 dropout=0,\n",
    "                 channel_mult=(1, 2, 2, 2),\n",
    "                 conv_resample=True,\n",
    "                 num_heads=4):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.cond_channels = cond_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions,\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult,\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # time embedding\n",
    "        time_emb_dim = model_channels * 4\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # down blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1))\n",
    "        ])\n",
    "        down_block_channels = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [ResidualBlock(ch, model_channels * mult, time_emb_dim, cond_channels, dropout)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                down_block_channels.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                self.down_blocks.append(TimestepEmbedSequential(DownSample(ch, conv_resample)))\n",
    "                down_block_channels.append(ch)\n",
    "                ds *= 2\n",
    "        \n",
    "        # middle blocks\n",
    "        self.middle_blocks = TimestepEmbedSequential(\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cond_channels, dropout),\n",
    "            AttentionBlock(ch, num_heads),\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cond_channels, dropout)\n",
    "        )\n",
    "        \n",
    "        # up blocks\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for level, mult in enumerate(channel_mult[::-1]):\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                layers = [\n",
    "                    ResidualBlock(ch + down_block_channels.pop(), model_channels * mult, time_emb_dim, cond_channels, dropout)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                if level != len(channel_mult) - 1 and i == num_res_blocks:\n",
    "                    layers.append(UpSample(ch, conv_resample))\n",
    "                    ds //= 2\n",
    "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                \n",
    "        self.out = nn.Sequential(\n",
    "            norm_layer(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, timesteps, cond_img, mask):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x H x W] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param cond_img: a [N x cond_C x H x W] Tensor of conditional images.\n",
    "        :param mask: a 1-D batch of conditioned/unconditioned.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        # time step embedding\n",
    "        t_emb = self.time_emb(timestep_embedding(timesteps, dim=self.model_channels))\n",
    "        \n",
    "        # down step\n",
    "        h = x\n",
    "        for module in self.down_blocks:\n",
    "            if cond_img.shape[2:] != h.shape[2:]:\n",
    "                cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "            h = module(h, t_emb, cond_img, mask)\n",
    "            hs.append(h)\n",
    "        # mid stage\n",
    "        if cond_img.shape[2:] != h.shape[2:]:\n",
    "            cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "        h = self.middle_blocks(h, t_emb, cond_img, mask)\n",
    "        \n",
    "        # up stage\n",
    "        for module in self.up_blocks:\n",
    "            h_skip = hs.pop()\n",
    "            \n",
    "            if h.shape[2:] != h_skip.shape[2:]:\n",
    "                h = F.interpolate(h, size=h_skip.shape[2:], mode='nearest')\n",
    "\n",
    "            if cond_img.shape[2:] != h.shape[2:]:\n",
    "                cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "\n",
    "            cat_in = torch.cat([h, h_skip], dim=1)\n",
    "            h = module(cat_in, t_emb, cond_img, mask)\n",
    "        \n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta schedule\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    betas = torch.sigmoid(betas) / (betas.max() - betas.min()) * (0.02 - betas.min()) / 10\n",
    "    return betas\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000,\n",
    "        beta_schedule='linear',\n",
    "    ):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            betas = sigmoid_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule {beta_schedule}')\n",
    "        \n",
    "        self.betas = betas\n",
    "        \n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "        \n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "        \n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]])\n",
    "        )\n",
    "        \n",
    "        self.posterior_mean_coef1 = (\n",
    "            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    # get the param of given timestep t\n",
    "    def _extract(self, a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.to(t.device).gather(0, t).float()\n",
    "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "        return out\n",
    "    \n",
    "    # forward diffusion : q(x_t | x_0)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    # mean and variance of q(x_t | x_0)\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "    \n",
    "    # mean and variance of diffusion posterior: q(x_{t-1} | x_t, x_0)\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "    \n",
    "    # compute x_0 from x_t and pred noise: reverse of q_sample\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    # compute predicted mean and variance of p(x_{t-1} | x_t) \n",
    "    def p_mean_variance(self, model, x_t, t, cond_img, w, clip_denoised=True):\n",
    "        device = next(model.parameters()).device\n",
    "        batch_size = x_t.shape[0]\n",
    "        \n",
    "        # noise prediction from model\n",
    "        pred_noise_cond = model(x_t, t, cond_img, torch.ones(batch_size).int().to(device))\n",
    "        pred_noise_uncond = model(x_t, t, cond_img, torch.zeros(batch_size).int().to(device))\n",
    "        pred_noise = (1 + w) * pred_noise_cond - w * pred_noise_uncond\n",
    "        \n",
    "        # get predicted x_0\n",
    "        x_recon = self.predict_start_from_noise(x_t, t, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x_recon = torch.clamp(x_recon, min=-1., max=1.)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior_mean_variance(x_recon, x_t, t)\n",
    "        \n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "    \n",
    "    # denoise step: sample x_{t-1} from x_t and pred noise\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t, cond_img, w, clip_denoised=True):\n",
    "        # pred mean and variance\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t, cond_img, w, clip_denoised=clip_denoised)\n",
    "        \n",
    "        noise = torch.randn_like(x_t)\n",
    "        # no noise when t = 0 \n",
    "        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n",
    "        # compute x_{t-1}\n",
    "        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img\n",
    "    \n",
    "    # denoise : reverse diffusion\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape, cond_img, w=2, clip_denoised=True):\n",
    "        batch_size = shape[0]\n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        # start from pure noise\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "        for i in tqdm(reversed(range(0, self.timesteps)), desc='sampling loop time step', total=self.timesteps):\n",
    "            img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long), cond_img, w, clip_denoised)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "    \n",
    "    # sample new images\n",
    "    @torch.no_grad\n",
    "    def sample(self, model, image_size, cond_img, batch_size=8, channels=3, w=2, clip_denoised=True):\n",
    "        return self.p_sample_loop(model, (batch_size, channels, image_size, image_size), cond_img, w, clip_denoised)\n",
    "    \n",
    "    # use ddim to sample\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        image_size,\n",
    "        cond_img,\n",
    "        batch_size=8,\n",
    "        channels=3,\n",
    "        ddim_timesteps=50,\n",
    "        w=2,\n",
    "        ddim_discr_method=\"uniform\",\n",
    "        ddim_eta=0.0,\n",
    "        clip_denoised=True):\n",
    "        \n",
    "        # make ddim timestep sequence\n",
    "        if ddim_discr_method == 'uniform':\n",
    "            c = self.timesteps // ddim_timesteps\n",
    "            ddim_timestep_seq = np.asarray(list(range(0, self.timesteps, c)))\n",
    "        elif ddim_discr_method == 'quad':\n",
    "            ddim_timestep_seq = (\n",
    "                (np.linspace(0, np.sqrt(self.timesteps * .8), ddim_timesteps)) ** 2\n",
    "            ).astype(int)\n",
    "        else:\n",
    "            raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
    "        # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "        ddim_timestep_seq = ddim_timestep_seq + 1\n",
    "        # previous sequence\n",
    "        ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        # start from pure noise (for each example in the batch)\n",
    "        sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)\n",
    "        seq_img = [sample_img.cpu().numpy()]   \n",
    "        \n",
    "        for i in tqdm(reversed(range(0, ddim_timesteps)), desc='sampling loop time step', total=ddim_timesteps):\n",
    "            t = torch.full((batch_size,), ddim_timestep_seq[i], device=device, dtype=torch.long)\n",
    "            prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=device, dtype=torch.long)\n",
    "            \n",
    "            # 1. get current and previous alpha_cumprod\n",
    "            alpha_cumprod_t = self._extract(self.alphas_cumprod, t, sample_img.shape)\n",
    "            alpha_cumprod_t_prev = self._extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
    "    \n",
    "            # 2. predict noise using model\n",
    "            pred_noise_cond = model(sample_img, t, cond_img, torch.ones(batch_size).int().cuda())\n",
    "            pred_noise_uncond = model(sample_img, t, cond_img, torch.zeros(batch_size).int().cuda())\n",
    "            pred_noise = (1+w)*pred_noise_cond - w*pred_noise_uncond\n",
    "            \n",
    "            # 3. get the predicted x_0\n",
    "            pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "            if clip_denoised:\n",
    "                pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
    "            \n",
    "            # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "            # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "            sigmas_t = ddim_eta * torch.sqrt(\n",
    "                (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
    "            \n",
    "            # 5. compute \"direction pointing to x_t\" of formula (12)\n",
    "            pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
    "            \n",
    "            # 6. compute x_{t-1} of formula (12)\n",
    "            x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
    "\n",
    "            sample_img = x_prev\n",
    "\n",
    "        return sample_img.cpu().numpy()\n",
    "    \n",
    "    # compute train losses\n",
    "    def train_losses(self, model, x_start, t, cond_img, mask_c):\n",
    "        # generate random noise\n",
    "        noise = torch.randn_like(x_start)\n",
    "        # get x_t\n",
    "        x_noisy = self.q_sample(x_start, t, noise=noise)\n",
    "        predicted_noise = model(x_noisy, t, cond_img, mask_c)\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "        return loss, predicted_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, conditional_offset=5):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.conditional_offset = conditional_offset\n",
    "        self.regression_csv_path = os.path.join(data_dir, \"regression_params.csv\")\n",
    "        self.cond_images = []\n",
    "        self.target_images = []\n",
    "        self.reg_data = None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        files = sorted([os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir) if f.endswith('.mpy')])\n",
    "        for file in files:\n",
    "            with open(file, 'rb') as f:\n",
    "                images = pickle.load(f)\n",
    "                if isinstance(images, list):\n",
    "                    images = np.array(images)\n",
    "                    \n",
    "                for img_idx in range(len(images) - self.conditional_offset):\n",
    "                    self.cond_images.append(images[img_idx])\n",
    "                    self.target_images.append(images[img_idx + self.conditional_offset])\n",
    "                    \n",
    "        headers = [\"p_h\", \"a\", \"c_1\", \"c_2\"]\n",
    "        self.reg_data = pd.read_csv(self.regression_csv_path, names=headers, index_col=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cond_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # collection_idx, image_idx = self._get_indices(idx)\n",
    "        cond_image = self.cond_images[idx]\n",
    "        image = self.target_images[idx]\n",
    "        reg_params = self.reg_data.loc[idx // 15].to_numpy()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            cond_image = self.transform(cond_image)\n",
    "\n",
    "        return image, cond_image, reg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "DATA_DIR = \"C:/Users/Anirbit/Desktop/MSc/Ind Project/Msc-Project/data/simulated_bin_frames\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(64)\n",
    "])\n",
    "\n",
    "dataset = ConditionalImageDataset(DATA_DIR, transform=transform)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for image feature extraction\n",
    "class CNNRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNRegressionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNRegressionModel()\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0072460095927288585\n",
      "Epoch 2/20, Loss: 0.004443303343247284\n",
      "Epoch 3/20, Loss: 0.0033913400045342064\n",
      "Epoch 4/20, Loss: 0.002344317649575797\n",
      "Epoch 5/20, Loss: 0.0014983867480292577\n",
      "Epoch 6/20, Loss: 0.0008822234627917748\n",
      "Epoch 7/20, Loss: 0.0004795504805356772\n",
      "Epoch 8/20, Loss: 0.0002643631976918021\n",
      "Epoch 9/20, Loss: 0.0001475237493816001\n",
      "Epoch 10/20, Loss: 0.00012750374431065708\n",
      "Epoch 11/20, Loss: 0.00010113181054872588\n",
      "Epoch 12/20, Loss: 8.577129235411782e-05\n",
      "Epoch 13/20, Loss: 6.754561532447538e-05\n",
      "Epoch 14/20, Loss: 6.407163056626979e-05\n",
      "Epoch 15/20, Loss: 6.907817541187714e-05\n",
      "Epoch 16/20, Loss: 4.7624761546930216e-05\n",
      "Epoch 17/20, Loss: 3.785977998466644e-05\n",
      "Epoch 18/20, Loss: 4.2533276676046874e-05\n",
      "Epoch 19/20, Loss: 5.457515428795225e-05\n",
      "Epoch 20/20, Loss: 8.085578015611232e-05\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for _, cond_img, target_params in train_loader:\n",
    "        \n",
    "        # print(torch.tensor(np.array(target_params)))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cond_img = cond_img.float().to(device)\n",
    "        target_params = target_params.float().to(device)\n",
    "        \n",
    "        outputs = model(cond_img)\n",
    "        loss = criterion(outputs, target_params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Pred: [0.26392904 0.0561601  0.10273723 0.13695803], Ground Truth: [0.26392904 0.0561601  0.10273723 0.13695803]\n",
      "Model Pred: [0.2632153  0.11162888 0.11703021 0.16113514], Ground Truth: [0.2632153  0.11162888 0.11703021 0.16113514]\n",
      "Model Pred: [0.2980793  0.03860021 0.06069222 0.01075436], Ground Truth: [0.2980793  0.03860021 0.06069222 0.01075436]\n",
      "Model Pred: [0.23170722 0.09620376 0.02492217 0.06899657], Ground Truth: [0.23170722 0.09620376 0.02492217 0.06899657]\n",
      "Model Pred: [0.29956606 0.09258951 0.06901851 0.20798907], Ground Truth: [0.29956606 0.09258951 0.06901851 0.20798907]\n",
      "Model Pred: [0.22392102 0.09669428 0.02083882 0.072037  ], Ground Truth: [0.22392102 0.09669428 0.02083882 0.072037  ]\n",
      "Model Pred: [0.27051306 0.06587572 0.04179238 0.00372292], Ground Truth: [0.27051306 0.06587572 0.04179238 0.00372292]\n",
      "Model Pred: [0.3417225  0.09212939 0.00485243 0.22621259], Ground Truth: [0.3417225  0.09212939 0.00485243 0.22621259]\n",
      "Model Pred: [0.2073699  0.02798316 0.09382714 0.09186655], Ground Truth: [0.2073699  0.02798316 0.09382714 0.09186655]\n",
      "Model Pred: [0.34524882 0.07311828 0.02820692 0.11360835], Ground Truth: [0.34524882 0.07311828 0.02820692 0.11360835]\n",
      "Model Pred: [0.29818296 0.00298397 0.04192276 0.00270152], Ground Truth: [0.29818296 0.00298397 0.04192276 0.00270152]\n",
      "Model Pred: [0.20673487 0.07891403 0.09074234 0.14336222], Ground Truth: [0.20673487 0.07891403 0.09074234 0.14336222]\n",
      "Model Pred: [0.3394314  0.08246244 0.03533514 0.15120602], Ground Truth: [0.3394314  0.08246244 0.03533514 0.15120602]\n",
      "Model Pred: [0.3349775  0.0959442  0.05105343 0.41439232], Ground Truth: [0.3349775  0.0959442  0.05105343 0.41439232]\n",
      "Model Pred: [0.2073699  0.02798316 0.09382714 0.09186655], Ground Truth: [0.2073699  0.02798316 0.09382714 0.09186655]\n",
      "Model Pred: [0.3153149  0.05221774 0.04887712 0.26165524], Ground Truth: [0.3153149  0.05221774 0.04887712 0.26165524]\n",
      "Model Pred: [0.34016064 0.06060579 0.02231092 0.3424926 ], Ground Truth: [0.34016064 0.06060579 0.02231092 0.3424926 ]\n",
      "Model Pred: [0.29266873 0.13989863 0.09479137 0.3235748 ], Ground Truth: [0.29266873 0.13989863 0.09479137 0.3235748 ]\n",
      "Model Pred: [0.26211247 0.01996767 0.09439237 0.10708103], Ground Truth: [0.26211247 0.01996767 0.09439237 0.10708103]\n",
      "Model Pred: [0.3417225  0.09212939 0.00485243 0.22621259], Ground Truth: [0.3417225  0.09212939 0.00485243 0.22621259]\n",
      "Model Pred: [0.27402055 0.00138818 0.03027553 0.04284614], Ground Truth: [0.27402055 0.00138818 0.03027553 0.04284614]\n",
      "Model Pred: [0.34016064 0.06060579 0.02231092 0.3424926 ], Ground Truth: [0.34016064 0.06060579 0.02231092 0.3424926 ]\n",
      "Model Pred: [0.25380003 0.05355377 0.05564722 0.06464208], Ground Truth: [0.25380003 0.05355377 0.05564722 0.06464208]\n",
      "Model Pred: [0.24359296 0.09382644 0.04807245 0.18443882], Ground Truth: [0.24359296 0.09382644 0.04807245 0.18443882]\n",
      "Model Pred: [0.2835545  0.0849907  0.0481415  0.00849345], Ground Truth: [0.2835545  0.0849907  0.0481415  0.00849345]\n",
      "Model Pred: [0.206447   0.05038419 0.05287089 0.02020232], Ground Truth: [0.206447   0.05038419 0.05287089 0.02020232]\n",
      "Model Pred: [0.31107408 0.03518789 0.1166182  0.30653667], Ground Truth: [0.31107408 0.03518789 0.1166182  0.30653667]\n",
      "Model Pred: [0.34302282 0.07140385 0.02646397 0.11461981], Ground Truth: [0.34302282 0.07140385 0.02646397 0.11461981]\n",
      "Model Pred: [0.20673487 0.07891403 0.09074234 0.14336222], Ground Truth: [0.20673487 0.07891403 0.09074234 0.14336222]\n",
      "Model Pred: [0.23579217 0.07347921 0.03702115 0.07922385], Ground Truth: [0.23579217 0.07347921 0.03702115 0.07922385]\n",
      "Model Pred: [0.25096107 0.05580845 0.05553816 0.05863295], Ground Truth: [0.25096107 0.05580845 0.05553816 0.05863295]\n",
      "Model Pred: [0.34103614 0.07754057 0.01974587 0.00385348], Ground Truth: [0.34103614 0.07754057 0.01974587 0.00385348]\n",
      "Model Pred: [0.2577917  0.11641532 0.11796303 0.0667811 ], Ground Truth: [0.2577917  0.11641532 0.11796303 0.0667811 ]\n",
      "Model Pred: [0.206447   0.05038419 0.05287089 0.02020232], Ground Truth: [0.206447   0.05038419 0.05287089 0.02020232]\n",
      "Model Pred: [0.22372809 0.01401568 0.06494339 0.11026821], Ground Truth: [0.22372809 0.01401568 0.06494339 0.11026821]\n",
      "Model Pred: [0.33557004 0.00832635 0.10612987 0.13033262], Ground Truth: [0.33557004 0.00832635 0.10612987 0.13033262]\n",
      "Model Pred: [0.2773624  0.09551717 0.03864689 0.23326698], Ground Truth: [0.2773624  0.09551717 0.03864689 0.23326698]\n",
      "Model Pred: [0.25843614 0.1398024  0.04969693 0.13029024], Ground Truth: [0.25843614 0.1398024  0.04969693 0.13029024]\n",
      "Model Pred: [0.31711772 0.15001272 0.0464624  0.26528016], Ground Truth: [0.31711772 0.15001272 0.0464624  0.26528016]\n",
      "Model Pred: [0.27051306 0.06587572 0.04179238 0.00372292], Ground Truth: [0.27051306 0.06587572 0.04179238 0.00372292]\n",
      "Model Pred: [0.25594595 0.0771536  0.08389328 0.20293278], Ground Truth: [0.25594595 0.0771536  0.08389328 0.20293278]\n",
      "Model Pred: [0.21151902 0.05878294 0.05241172 0.03791148], Ground Truth: [0.21151902 0.05878294 0.05241172 0.03791148]\n",
      "Model Pred: [0.318083   0.03566752 0.10715259 0.31969348], Ground Truth: [0.318083   0.03566752 0.10715259 0.31969348]\n",
      "Model Pred: [0.3121643  0.10627151 0.08901452 0.20529053], Ground Truth: [0.3121643  0.10627151 0.08901452 0.20529053]\n",
      "Model Pred: [0.20673487 0.07891403 0.09074234 0.14336222], Ground Truth: [0.20673487 0.07891403 0.09074234 0.14336222]\n",
      "Model Pred: [0.27051306 0.06587572 0.04179238 0.00372292], Ground Truth: [0.27051306 0.06587572 0.04179238 0.00372292]\n",
      "Model Pred: [0.29341608 0.12219467 0.09942675 0.28307137], Ground Truth: [0.29341608 0.12219467 0.09942675 0.28307137]\n",
      "Model Pred: [ 0.3345733   0.07941553  0.02045906 -0.00062202], Ground Truth: [ 0.3345733   0.07941553  0.02045906 -0.00062202]\n",
      "Model Pred: [0.3349775  0.0959442  0.05105343 0.41439232], Ground Truth: [0.3349775  0.0959442  0.05105343 0.41439232]\n",
      "Model Pred: [0.23617816 0.08543493 0.03285088 0.08057236], Ground Truth: [0.23617816 0.08543493 0.03285088 0.08057236]\n",
      "Model Pred: [0.30800316 0.04873289 0.06774569 0.09840702], Ground Truth: [0.30800316 0.04873289 0.06774569 0.09840702]\n",
      "Model Pred: [0.25843614 0.1398024  0.04969693 0.13029024], Ground Truth: [0.25843614 0.1398024  0.04969693 0.13029024]\n",
      "Model Pred: [0.24359296 0.09382644 0.04807245 0.18443882], Ground Truth: [0.24359296 0.09382644 0.04807245 0.18443882]\n",
      "Model Pred: [0.28226358 0.0910247  0.03342465 0.2142078 ], Ground Truth: [0.28226358 0.0910247  0.03342465 0.2142078 ]\n",
      "Model Pred: [0.28926426 0.0785124  0.0121698  0.24873558], Ground Truth: [0.28926426 0.0785124  0.0121698  0.24873558]\n",
      "Model Pred: [0.22372809 0.01401568 0.06494339 0.11026821], Ground Truth: [0.22372809 0.01401568 0.06494339 0.11026821]\n",
      "Model Pred: [ 0.33741882  0.07479158  0.02069598 -0.00074541], Ground Truth: [ 0.33741882  0.07479158  0.02069598 -0.00074541]\n",
      "Model Pred: [0.31214845 0.10904153 0.05379729 0.24075255], Ground Truth: [0.31214845 0.10904153 0.05379729 0.24075255]\n",
      "Model Pred: [0.25651813 0.00976752 0.09234618 0.11428215], Ground Truth: [0.25651813 0.00976752 0.09234618 0.11428215]\n",
      "Model Pred: [0.23328972 0.01793525 0.08549075 0.12877128], Ground Truth: [0.23328972 0.01793525 0.08549075 0.12877128]\n",
      "Model Pred: [0.25843614 0.1398024  0.04969693 0.13029024], Ground Truth: [0.25843614 0.1398024  0.04969693 0.13029024]\n",
      "Model Pred: [0.31246278 0.03069504 0.12387323 0.31668177], Ground Truth: [0.31246278 0.03069504 0.12387323 0.31668177]\n",
      "Model Pred: [0.25380003 0.05355377 0.05564722 0.06464208], Ground Truth: [0.25380003 0.05355377 0.05564722 0.06464208]\n",
      "Model Pred: [0.27875173 0.00560188 0.01032928 0.2735401 ], Ground Truth: [0.27875173 0.00560188 0.01032928 0.2735401 ]\n",
      "Model Pred: [0.28357643 0.08245596 0.01646874 0.24553448], Ground Truth: [0.28357643 0.08245596 0.01646874 0.24553448]\n",
      "Model Pred: [0.20079091 0.11472902 0.10500288 0.13431785], Ground Truth: [0.20079091 0.11472902 0.10500288 0.13431785]\n",
      "Model Pred: [0.24661067 0.10903241 0.05859026 0.31972006], Ground Truth: [0.24661067 0.10903241 0.05859026 0.31972006]\n",
      "Model Pred: [0.3417225  0.09212939 0.00485243 0.22621259], Ground Truth: [0.3417225  0.09212939 0.00485243 0.22621259]\n",
      "Model Pred: [0.29209518 0.10293146 0.09645434 0.21330386], Ground Truth: [0.29209518 0.10293146 0.09645434 0.21330386]\n",
      "Model Pred: [0.2864598  0.08413655 0.0934443  0.0800524 ], Ground Truth: [0.2864598  0.08413655 0.0934443  0.0800524 ]\n",
      "Model Pred: [0.26048908 0.0355803  0.07204723 0.18943599], Ground Truth: [0.26048908 0.0355803  0.07204723 0.18943599]\n",
      "Model Pred: [0.34016064 0.06060579 0.02231092 0.3424926 ], Ground Truth: [0.34016064 0.06060579 0.02231092 0.3424926 ]\n",
      "Model Pred: [0.24107845 0.05903873 0.10787195 0.0502909 ], Ground Truth: [0.24107845 0.05903873 0.10787195 0.0502909 ]\n",
      "Model Pred: [0.32083982 0.10996121 0.09297714 0.23196772], Ground Truth: [0.32083982 0.10996121 0.09297714 0.23196772]\n",
      "Model Pred: [0.2560503  0.0141984  0.09301747 0.1058807 ], Ground Truth: [0.2560503  0.0141984  0.09301747 0.1058807 ]\n",
      "Test Loss: 0.0001952443171740015\n"
     ]
    }
   ],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for _, cond_img, reg_params in test_loader:\n",
    "            cond_img = cond_img.float().to(device)\n",
    "            reg_params = reg_params.float().to(device)\n",
    "            \n",
    "            outputs = model(cond_img)\n",
    "            \n",
    "            print(f\"Model Pred: {outputs.cpu().numpy()[0]}, Ground Truth: {outputs.cpu().numpy()[0]}\")\n",
    "            \n",
    "            loss = criterion(outputs, reg_params)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a DataLoader for the test dataset named test_loader\n",
    "# and a trained model named model\n",
    "\n",
    "test_loss = evaluate_model(model, test_loader)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Guided Diffusion Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "diff_model = Unet(\n",
    "    in_channels=1,\n",
    "    cond_channels=1,\n",
    "    model_channels=96,\n",
    "    out_channels=1,\n",
    "    channel_mult=(1, 2, 2),\n",
    "    attention_resolutions=[],\n",
    ")\n",
    "diff_model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "p_uncound = 0.2\n",
    "len_data = len(train_loader)\n",
    "time_end = time.time()\n",
    "timesteps=500\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "gaussian_diffusion = GaussianDiffusion(timesteps=500, beta_schedule='linear')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (images, cond_images, reg_params) in enumerate(train_loader):     \n",
    "        time_start = time_end\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        images = images.to(device).float()\n",
    "        cond_images = cond_images.to(device).float()\n",
    "        \n",
    "        # random generate mask\n",
    "        z_uncound = torch.rand(batch_size)\n",
    "        batch_mask = (z_uncound>p_uncound).int().to(device)\n",
    "        \n",
    "        # sample t uniformally for every example in the batch\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "        loss, outputs = gaussian_diffusion.train_losses(diff_model, images, t, cond_images, batch_mask)\n",
    "        \n",
    "        reg_output = model(outputs)\n",
    "        reg_loss = criterion(reg_output, reg_params)\n",
    "        \n",
    "        loss += reg_loss\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            time_end = time.time()\n",
    "            print(\"Epoch{}/{}\\t  Step{}/{}\\t Loss {:.4f}\\t Time {:.2f}\".format(epoch+1, epochs, step+1, len_data, loss.item(), time_end-time_start))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
