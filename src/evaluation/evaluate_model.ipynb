{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "        hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], \n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "class WildfirePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(WildfirePredictor, self).__init__()\n",
    "        self.convLstm = ConvLSTM(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=kernel_size, num_layers=num_layers, return_all_layers=False)\n",
    "        self.conv = nn.Conv2d(hidden_dim[-1], 1, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        layer_output_list, last_state_list = self.convLstm(x)\n",
    "        out = layer_output_list[0][:, -1, :, :, :]\n",
    "        out = self.conv(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wildfire_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wildfire_CNN, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),  # Dropout with a 50% drop probability\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIFFUSION - Classifier free guidance and Image conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta schedule\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    betas = torch.sigmoid(betas) / (betas.max() - betas.min()) * (0.02 - betas.min()) / 10\n",
    "    return betas\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=1000):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "    def forward(self, x, t_emb, c_emb, mask):\n",
    "        for layer in self:\n",
    "            if(isinstance(layer, TimestepBlock)):\n",
    "                x = layer(x, t_emb, c_emb, mask)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "\n",
    "def norm_layer(channels):\n",
    "    return nn.GroupNorm(32, channels)\n",
    "\n",
    "class ResidualBlock(TimestepBlock):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, cond_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            norm_layer(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.cond_conv = nn.Sequential(\n",
    "            nn.Conv2d(cond_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            norm_layer(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t, cond_img, mask):\n",
    "        h = self.conv1(x)\n",
    "        emb_t = self.time_emb(t)\n",
    "        emb_cond = self.cond_conv(cond_img) * mask[:, None, None, None]\n",
    "        h += (emb_t[:, :, None, None] + emb_cond)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        return h + self.shortcut(x)\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert channels % num_heads == 0\n",
    "        \n",
    "        self.norm = norm_layer(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q, k, v = qkv.reshape(B *  self.num_heads, -1, H * W).chunk(3, dim=1)\n",
    "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
    "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
    "        h = h.reshape(B, -1, H, W)\n",
    "        h = self.proj(h)\n",
    "        return h + x\n",
    "    \n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, padding=1, stride=2)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=2,\n",
    "                 cond_channels=1,\n",
    "                 model_channels=128,\n",
    "                 out_channels=2,\n",
    "                 num_res_blocks=2, \n",
    "                 attention_resolutions=(8, 16),\n",
    "                 dropout=0,\n",
    "                 channel_mult=(1, 2, 2, 2),\n",
    "                 conv_resample=True,\n",
    "                 num_heads=4):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.cond_channels = cond_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions,\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult,\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # time embedding\n",
    "        time_emb_dim = model_channels * 4\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # down blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1))\n",
    "        ])\n",
    "        down_block_channels = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [ResidualBlock(ch, model_channels * mult, time_emb_dim, cond_channels, dropout)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                down_block_channels.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                self.down_blocks.append(TimestepEmbedSequential(DownSample(ch, conv_resample)))\n",
    "                down_block_channels.append(ch)\n",
    "                ds *= 2\n",
    "        \n",
    "        # middle blocks\n",
    "        self.middle_blocks = TimestepEmbedSequential(\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cond_channels, dropout),\n",
    "            AttentionBlock(ch, num_heads),\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cond_channels, dropout)\n",
    "        )\n",
    "        \n",
    "        # up blocks\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for level, mult in enumerate(channel_mult[::-1]):\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                layers = [\n",
    "                    ResidualBlock(ch + down_block_channels.pop(), model_channels * mult, time_emb_dim, cond_channels, dropout)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "                if level != len(channel_mult) - 1 and i == num_res_blocks:\n",
    "                    layers.append(UpSample(ch, conv_resample))\n",
    "                    ds //= 2\n",
    "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                \n",
    "        self.out = nn.Sequential(\n",
    "            norm_layer(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, timesteps, cond_img, mask):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x H x W] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param cond_img: a [N x cond_C x H x W] Tensor of conditional images.\n",
    "        :param mask: a 1-D batch of conditioned/unconditioned.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        # time step embedding\n",
    "        t_emb = self.time_emb(timestep_embedding(timesteps, dim=self.model_channels))\n",
    "        \n",
    "        # down step\n",
    "        h = x\n",
    "        for module in self.down_blocks:\n",
    "            if cond_img.shape[2:] != h.shape[2:]:\n",
    "                cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "            h = module(h, t_emb, cond_img, mask)\n",
    "            hs.append(h)\n",
    "        # mid stage\n",
    "        if cond_img.shape[2:] != h.shape[2:]:\n",
    "            cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "        h = self.middle_blocks(h, t_emb, cond_img, mask)\n",
    "        \n",
    "        # up stage\n",
    "        for module in self.up_blocks:\n",
    "            h_skip = hs.pop()\n",
    "            \n",
    "            if h.shape[2:] != h_skip.shape[2:]:\n",
    "                h = F.interpolate(h, size=h_skip.shape[2:], mode='nearest')\n",
    "\n",
    "            if cond_img.shape[2:] != h.shape[2:]:\n",
    "                cond_img = F.interpolate(cond_img, size=h.shape[2:], mode='nearest')\n",
    "\n",
    "            cat_in = torch.cat([h, h_skip], dim=1)\n",
    "            h = module(cat_in, t_emb, cond_img, mask)\n",
    "        \n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000,\n",
    "        beta_schedule='linear',\n",
    "    ):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            betas = sigmoid_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule {beta_schedule}')\n",
    "        \n",
    "        self.betas = betas\n",
    "        \n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "        \n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "        \n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]])\n",
    "        )\n",
    "        \n",
    "        self.posterior_mean_coef1 = (\n",
    "            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    # get the param of given timestep t\n",
    "    def _extract(self, a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.to(t.device).gather(0, t).float()\n",
    "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "        return out\n",
    "    \n",
    "    # forward diffusion : q(x_t | x_0)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    # mean and variance of q(x_t | x_0)\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "    \n",
    "    # mean and variance of diffusion posterior: q(x_{t-1} | x_t, x_0)\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "    \n",
    "    # compute x_0 from x_t and pred noise: reverse of q_sample\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    # compute predicted mean and variance of p(x_{t-1} | x_t) \n",
    "    def p_mean_variance(self, model, x_t, t, cond_img, w, clip_denoised=True):\n",
    "        device = next(model.parameters()).device\n",
    "        batch_size = x_t.shape[0]\n",
    "        \n",
    "        # noise prediction from model\n",
    "        pred_noise_cond = model(x_t, t, cond_img, torch.ones(batch_size).int().to(device))\n",
    "        pred_noise_uncond = model(x_t, t, cond_img, torch.zeros(batch_size).int().to(device))\n",
    "        pred_noise = (1 + w) * pred_noise_cond - w * pred_noise_uncond\n",
    "        \n",
    "        # get predicted x_0\n",
    "        x_recon = self.predict_start_from_noise(x_t, t, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x_recon = torch.clamp(x_recon, min=-1., max=1.)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior_mean_variance(x_recon, x_t, t)\n",
    "        \n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "    \n",
    "    # denoise step: sample x_{t-1} from x_t and pred noise\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t, cond_img, w, clip_denoised=True):\n",
    "        # pred mean and variance\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t, cond_img, w, clip_denoised=clip_denoised)\n",
    "        \n",
    "        noise = torch.randn_like(x_t)\n",
    "        # no noise when t = 0 \n",
    "        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n",
    "        # compute x_{t-1}\n",
    "        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img\n",
    "    \n",
    "    # denoise : reverse diffusion\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape, cond_img, w=2, clip_denoised=True):\n",
    "        batch_size = shape[0]\n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        # start from pure noise\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "        for i in tqdm(reversed(range(0, self.timesteps)), desc='sampling loop time step', total=self.timesteps):\n",
    "            img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long), cond_img, w, clip_denoised)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "    \n",
    "    # sample new images\n",
    "    @torch.no_grad\n",
    "    def sample(self, model, image_size, cond_img, batch_size=8, channels=3, w=2, clip_denoised=True):\n",
    "        return self.p_sample_loop(model, (batch_size, channels, image_size, image_size), cond_img, w, clip_denoised)\n",
    "    \n",
    "    # use ddim to sample\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        image_size,\n",
    "        cond_img,\n",
    "        batch_size=8,\n",
    "        channels=3,\n",
    "        ddim_timesteps=50,\n",
    "        w=2,\n",
    "        ddim_discr_method=\"uniform\",\n",
    "        ddim_eta=0.0,\n",
    "        clip_denoised=True):\n",
    "        \n",
    "        # make ddim timestep sequence\n",
    "        if ddim_discr_method == 'uniform':\n",
    "            c = self.timesteps // ddim_timesteps\n",
    "            ddim_timestep_seq = np.asarray(list(range(0, self.timesteps, c)))\n",
    "        elif ddim_discr_method == 'quad':\n",
    "            ddim_timestep_seq = (\n",
    "                (np.linspace(0, np.sqrt(self.timesteps * .8), ddim_timesteps)) ** 2\n",
    "            ).astype(int)\n",
    "        else:\n",
    "            raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
    "        # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "        ddim_timestep_seq = ddim_timestep_seq + 1\n",
    "        # previous sequence\n",
    "        ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        # start from pure noise (for each example in the batch)\n",
    "        sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)\n",
    "        seq_img = [sample_img.cpu().numpy()]   \n",
    "        \n",
    "        for i in tqdm(reversed(range(0, ddim_timesteps)), desc='sampling loop time step', total=ddim_timesteps):\n",
    "            t = torch.full((batch_size,), ddim_timestep_seq[i], device=device, dtype=torch.long)\n",
    "            prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=device, dtype=torch.long)\n",
    "            \n",
    "            # 1. get current and previous alpha_cumprod\n",
    "            alpha_cumprod_t = self._extract(self.alphas_cumprod, t, sample_img.shape)\n",
    "            alpha_cumprod_t_prev = self._extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
    "    \n",
    "            # 2. predict noise using model\n",
    "            pred_noise_cond = model(sample_img, t, cond_img, torch.ones(batch_size).int().cuda())\n",
    "            pred_noise_uncond = model(sample_img, t, cond_img, torch.zeros(batch_size).int().cuda())\n",
    "            pred_noise = (1+w)*pred_noise_cond - w*pred_noise_uncond\n",
    "            \n",
    "            # 3. get the predicted x_0\n",
    "            pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "            if clip_denoised:\n",
    "                pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
    "            \n",
    "            # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "            # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "            sigmas_t = ddim_eta * torch.sqrt(\n",
    "                (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
    "            \n",
    "            # 5. compute \"direction pointing to x_t\" of formula (12)\n",
    "            pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
    "            \n",
    "            # 6. compute x_{t-1} of formula (12)\n",
    "            x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
    "\n",
    "            sample_img = x_prev\n",
    "\n",
    "        return sample_img.cpu().numpy()\n",
    "    \n",
    "    # compute train losses\n",
    "    def train_losses(self, model, x_start, t, cond_img, mask_c):\n",
    "        # generate random noise\n",
    "        noise = torch.randn_like(x_start)\n",
    "        # get x_t\n",
    "        x_noisy = self.q_sample(x_start, t, noise=noise)\n",
    "        predicted_noise = model(x_noisy, t, cond_img, mask_c)\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION - COMPARE DIFFUSION PERFORMANCE TO BASELINE CONVLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# baseline_model = WildfirePredictor(input_dim=1, hidden_dim=[64, 64, 64], kernel_size=[3, 3, 3], num_layers=3)\n",
    "# baseline_model.load_state_dict(torch.load(\"C:/Users/Anirbit/Desktop/MSc/Ind Project/Msc-Project/results/ckpts/baseline_seq_convlstm_64x64.pt\"))\n",
    "# baseline_model.to(device)\n",
    "\n",
    "baseline_model = Wildfire_CNN()\n",
    "baseline_model.load_state_dict(torch.load(\"C:/Users/Anirbit/Desktop/MSc/Ind Project/Msc-Project/results/ckpts/baseline_cnn_64x64_simData.pt\"))\n",
    "baseline_model.to(device)\n",
    "\n",
    "diffusion_model = torch.load('C:/Users/Anirbit/Desktop/MSc/Ind Project/Msc-Project/results/ckpts/gde_ensemble_model_4_64x64_simData.h5')\n",
    "diffusion_model.to(device)\n",
    "\n",
    "diffusion_model_reg = torch.load('C:/Users/Anirbit/Desktop/MSc/Ind Project/Msc-Project/results/ckpts/gde_ensemble_model_4_64x64_simData_regGuided_noDataAug.h5')\n",
    "diffusion_model_reg.to(device)\n",
    "\n",
    "gaussian_diffusion = GaussianDiffusion(timesteps=500, beta_schedule='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondSeqImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, conditional_offset=5):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.conditional_offset = conditional_offset\n",
    "        self.cond_images = []\n",
    "        self.target_images = []\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        files = sorted([os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir) if f.endswith('.mpy')])\n",
    "        for file in files:\n",
    "            with open(file, 'rb') as f:\n",
    "                images = pickle.load(f)\n",
    "                if isinstance(images, list):\n",
    "                    images = np.array(images)\n",
    "                    \n",
    "                for img_idx in range(len(images) - self.conditional_offset):\n",
    "                    self.cond_images.append(images[img_idx])\n",
    "                    self.target_images.append(images[img_idx + self.conditional_offset])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cond_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # collection_idx, image_idx = self._get_indices(idx)\n",
    "        cond_image = self.cond_images[idx]\n",
    "        image = self.target_images[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            cond_image = self.transform(cond_image)\n",
    "\n",
    "        return image, cond_image\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,)), \n",
    "    transforms.Resize(64)\n",
    "])\n",
    "\n",
    "dataset_2 = CondSeqImageDataset(\"C:/Users/Anirbit/Desktop/MSc/Ind Project/Msc-Project/data/simulated_bin_frames\", transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(dataset_2))\n",
    "test_size = len(dataset_2) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset_2, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x2000 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_convlstm_predictions(model, cond_images, target_images):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    inputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(cond_images)\n",
    "        predictions.append(output.cpu())\n",
    "        ground_truths.append(target_images.cpu())\n",
    "        inputs.append(cond_images.cpu())\n",
    "\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    ground_truths = torch.cat(ground_truths, dim=0)\n",
    "    inputs = torch.cat(inputs, dim=0)\n",
    "\n",
    "    return predictions, ground_truths, inputs\n",
    "\n",
    "def generate_cnn_predictions(model, cond_images, target_images):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    inputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(cond_images)\n",
    "        predictions.append(output.cpu())\n",
    "        ground_truths.append(target_images.cpu())\n",
    "        inputs.append(cond_images.cpu())\n",
    "\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    ground_truths = torch.cat(ground_truths, dim=0)\n",
    "    inputs = torch.cat(inputs, dim=0)\n",
    "\n",
    "    return predictions, ground_truths, inputs\n",
    "\n",
    "def plot_cnn_predictions(predictions, ground_truths, inputs, num_images=5, save=False):\n",
    "    fig, axs = plt.subplots(num_images, 3, figsize=(18, 20))\n",
    "    for i in range(num_images):\n",
    "        input_img = inputs[i].squeeze(0)\n",
    "        axs[i, 0].imshow(input_img, cmap='gray')\n",
    "        axs[i, 0].set_title(f'Input')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # Plot the predicted image\n",
    "        pred_img = predictions[i].squeeze(0)\n",
    "        axs[i, 1].imshow(pred_img, cmap='gray')\n",
    "        axs[i, 1].set_title('Predicted')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "        # Plot the ground truth image\n",
    "        true_img = ground_truths[i].squeeze(0)\n",
    "        axs[i, 2].imshow(true_img, cmap='gray')\n",
    "        axs[i, 2].set_title('Ground Truth')\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(\"lstm_output.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_lstm_predictions(predictions, ground_truths, inputs, num_images=5, save=False):\n",
    "    fig, axs = plt.subplots(num_images, 6, figsize=(18, 20))\n",
    "    for i in range(num_images):\n",
    "        # Plot the 4 input images\n",
    "        for j in range(4):\n",
    "            input_img = inputs[i, j].squeeze(0)\n",
    "            axs[i, j].imshow(input_img, cmap='gray')\n",
    "            axs[i, j].set_title(f'Input {j+1}')\n",
    "            axs[i, j].axis('off')\n",
    "\n",
    "        # Plot the predicted image\n",
    "        pred_img = predictions[i].squeeze(0)\n",
    "        axs[i, 4].imshow(pred_img, cmap='gray')\n",
    "        axs[i, 4].set_title('Predicted')\n",
    "        axs[i, 4].axis('off')\n",
    "\n",
    "        # Plot the ground truth image\n",
    "        true_img = ground_truths[i].squeeze(0)\n",
    "        axs[i, 5].imshow(true_img, cmap='gray')\n",
    "        axs[i, 5].set_title('Ground Truth')\n",
    "        axs[i, 5].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(\"lstm_output.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data_iter = iter(test_loader)\n",
    "target_img, cond_img = next(data_iter)\n",
    "\n",
    "# Ensure images are on the correct device and of type float32\n",
    "target_img = target_img.to(device).float()\n",
    "cond_img = cond_img.to(device).float()\n",
    "\n",
    "preds, ground_truths, inputs = generate_cnn_predictions(baseline_model, cond_img, target_img)\n",
    "plot_cnn_predictions(preds, ground_truths, inputs, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import gaussian_filter, binary_opening, binary_closing\n",
    "\n",
    "def adaptive_threshold(image, sigma=1.0, morph_size=3):\n",
    "    # Normalize image to range [0, 1] from [-1, 1]\n",
    "    normalized_image = (image + 1) / 2.0\n",
    "    \n",
    "    # Apply Gaussian filter to smooth the image (optional, helps with noise)\n",
    "    smoothed_image = gaussian_filter(normalized_image, sigma=sigma)\n",
    "    \n",
    "    # Flatten the smoothed image\n",
    "    pixel_values = smoothed_image.flatten().reshape(-1, 1)\n",
    "    \n",
    "    # Perform K-means clustering with 2 clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "    kmeans.fit(pixel_values)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Determine which cluster is the light region\n",
    "    cluster_means = kmeans.cluster_centers_.flatten()\n",
    "    light_cluster = np.argmax(cluster_means)\n",
    "    \n",
    "    # Create a binary image based on the cluster labels\n",
    "    binary_image = np.where(labels == light_cluster, 1, 0)\n",
    "    binary_image = binary_image.reshape(normalized_image.shape)\n",
    "    \n",
    "    # Perform morphological operations to refine the regions\n",
    "    binary_image = binary_opening(binary_image, structure=np.ones((morph_size, morph_size)))\n",
    "    binary_image = binary_closing(binary_image, structure=np.ones((morph_size, morph_size)))\n",
    "    \n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "sampling loop time step:   4%|▎         | 18/500 [00:04<01:57,  4.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m generated_image_arr \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     generated_images \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_diffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set the desired batch size\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     generated_image_arr\u001b[38;5;241m.\u001b[39mappend(generated_images[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m     25\u001b[0m mean_preds \u001b[38;5;241m=\u001b[39m mean_prediction(generated_image_arr)\n",
      "File \u001b[1;32mc:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 133\u001b[0m, in \u001b[0;36mGaussianDiffusion.sample\u001b[1;34m(self, model, image_size, cond_img, batch_size, channels, w, clip_denoised)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, image_size, cond_img, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, clip_denoised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 126\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[1;34m(self, model, shape, cond_img, w, clip_denoised)\u001b[0m\n\u001b[0;32m    124\u001b[0m imgs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling loop time step\u001b[39m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps):\n\u001b[1;32m--> 126\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     imgs\u001b[38;5;241m.\u001b[39mappend(img\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m imgs\n",
      "File \u001b[1;32mc:\\Users\\Anirbit\\anaconda3\\envs\\mscproj\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 107\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[1;34m(self, model, x_t, t, cond_img, w, clip_denoised)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_sample\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, x_t, t, cond_img, w, clip_denoised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# pred mean and variance\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     model_mean, _, model_log_variance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x_t)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# no noise when t = 0 \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 92\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[1;34m(self, model, x_t, t, cond_img, w, clip_denoised)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# noise prediction from model\u001b[39;00m\n\u001b[0;32m     91\u001b[0m pred_noise_cond \u001b[38;5;241m=\u001b[39m model(x_t, t, cond_img, torch\u001b[38;5;241m.\u001b[39mones(batch_size)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m---> 92\u001b[0m pred_noise_uncond \u001b[38;5;241m=\u001b[39m model(x_t, t, cond_img, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     93\u001b[0m pred_noise \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m w) \u001b[38;5;241m*\u001b[39m pred_noise_cond \u001b[38;5;241m-\u001b[39m w \u001b[38;5;241m*\u001b[39m pred_noise_uncond\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# get predicted x_0\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mean_prediction(pred_arr):\n",
    "    imgs = np.array(pred_arr)\n",
    "    avg = np.mean(imgs, axis=0)\n",
    "    \n",
    "    return avg\n",
    "\n",
    "# Define the image size and channels (modify as per your requirements)\n",
    "image_size = target_img.shape[-1]\n",
    "channels = target_img.shape[1]\n",
    "\n",
    "# Call the sample method\n",
    "generated_image_arr = []\n",
    "for idx in range(10):\n",
    "    generated_images = gaussian_diffusion.sample(\n",
    "        model=diffusion_model,\n",
    "        image_size=image_size,\n",
    "        cond_img=cond_img,\n",
    "        batch_size=32,  # Set the desired batch size\n",
    "        channels=channels,\n",
    "        w=2,\n",
    "        clip_denoised=True\n",
    "    )\n",
    "    generated_image_arr.append(generated_images[-1].squeeze())\n",
    "\n",
    "mean_preds = mean_prediction(generated_image_arr)\n",
    "\n",
    "grid_size = int(np.ceil(np.sqrt(mean_preds.shape[0])))\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "\n",
    "# Remove axis for each subplot\n",
    "for ax in axes.flatten():\n",
    "    ax.axis('off')\n",
    "\n",
    "# Plot each image\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < mean_preds.shape[0]:\n",
    "        ax.imshow(adaptive_threshold(mean_preds[i], sigma=0.9, morph_size=3), cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 64, 64) (32, 1, 64, 64) (32, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "diffusion_outputs = generated_images[-1]\n",
    "cnnAE_outputs = preds.numpy()\n",
    "\n",
    "expected_outputs = ground_truths.numpy()\n",
    "\n",
    "print(diffusion_outputs.shape, cnnAE_outputs.shape, expected_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction error - MSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction error diffusion outputs - MSE: 0.00553008122369647.4f | MAE: 0.028236785903573036\n",
      "Reconstruction error CNN outputs - MSE: 0.002100895857438445 | MAE: 0.0036910015624016523\n"
     ]
    }
   ],
   "source": [
    "def calculate_errors(original_images, reconstructed_images):\n",
    "    mse = mean_squared_error(original_images.flatten(), reconstructed_images.flatten())\n",
    "    mae = mean_absolute_error(original_images.flatten(), reconstructed_images.flatten())\n",
    "    return mse, mae\n",
    "\n",
    "mse_diffusion, mae_diffusion = calculate_errors(expected_outputs, diffusion_outputs)\n",
    "mse_cnnAE, mae_cnnAE = calculate_errors(expected_outputs, cnnAE_outputs)\n",
    "\n",
    "print(f\"Reconstruction error diffusion outputs - MSE: {mse_diffusion} | MAE: {mae_diffusion}\")\n",
    "print(f\"Reconstruction error CNN outputs - MSE: {mse_cnnAE} | MAE: {mae_cnnAE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frechet Inception Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID diffusion outputs =  202.80608337136357\n",
      "FID CNN outputs  =  69.47160314097974\n"
     ]
    }
   ],
   "source": [
    "def preprocess_images(images):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    images = np.squeeze(images, axis=1)  # Remove the single channel dimension\n",
    "    return torch.stack([transform(img) for img in images])\n",
    "\n",
    "def get_inception_features(images, model, device):\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(images)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "def calculate_fid(real_images, generated_images, device):\n",
    "    # Load pre-trained InceptionV3 model\n",
    "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    model.fc = torch.nn.Identity()  # Remove the final classification layer\n",
    "\n",
    "    # Preprocess images\n",
    "    real_images = preprocess_images(real_images)\n",
    "    generated_images = preprocess_images(generated_images)\n",
    "\n",
    "    # Get InceptionV3 features\n",
    "    real_features = get_inception_features(real_images, model, device)\n",
    "    generated_features = get_inception_features(generated_images, model, device)\n",
    "\n",
    "    # Calculate mean and covariance matrices\n",
    "    mu_real, sigma_real = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_gen, sigma_gen = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n",
    "\n",
    "    # Calculate FID\n",
    "    ssdiff = np.sum((mu_real - mu_gen) ** 2.0)\n",
    "    covmean, _ = sqrtm(sigma_real.dot(sigma_gen), disp=False)\n",
    "    \n",
    "    # Numerical error might give slight imaginary component, discard if exists\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma_real + sigma_gen - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"FID diffusion outputs =  {calculate_fid(expected_outputs, diffusion_outputs, device)}\")\n",
    "print(f\"FID CNN outputs  =  {calculate_fid(expected_outputs, cnnAE_outputs, device)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSIM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM diffusion outputs =  0.3418541392928153\n",
      "SSIM CNN outputs  =  0.9788479131638226\n"
     ]
    }
   ],
   "source": [
    "def calculate_ssim(original_images, reconstructed_images, win_size=5):\n",
    "    \"\"\"\n",
    "    Calculate the mean SSIM between original and reconstructed images.\n",
    "\n",
    "    Parameters:\n",
    "    - original_images: numpy array of original images, shape (N, 64, 64)\n",
    "    - reconstructed_images: numpy array of reconstructed images, shape (N, 64, 64)\n",
    "    - win_size: int, optional. The side-length of the sliding window. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    - mean_ssim: float. Mean SSIM value over all image pairs.\n",
    "    \"\"\"\n",
    "    # Ensure the images are 2D (remove single channel dimension if present)\n",
    "    if original_images.shape[1] == 1:\n",
    "        original_images = np.squeeze(original_images, axis=1)\n",
    "    if reconstructed_images.shape[1] == 1:\n",
    "        reconstructed_images = np.squeeze(reconstructed_images, axis=1)\n",
    "    \n",
    "    # Calculate data range from the input images\n",
    "    data_range = original_images.max() - original_images.min()\n",
    "\n",
    "    ssim_values = [ssim(orig, recon, win_size=win_size, data_range=data_range) for orig, recon in zip(original_images, reconstructed_images)]\n",
    "    return np.mean(ssim_values)\n",
    "\n",
    "print(f\"SSIM diffusion outputs =  {calculate_ssim(expected_outputs, diffusion_outputs)}\")\n",
    "print(f\"SSIM CNN outputs  =  {calculate_ssim(expected_outputs, cnnAE_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
